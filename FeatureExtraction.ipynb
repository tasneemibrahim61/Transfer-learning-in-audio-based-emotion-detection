{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "io3Sm-lzNAnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the csv file\n",
        "# df = pd.read_csv(\"your_file.csv\")\n",
        "# texts = df[\"text_column\"].astype(str).tolist()\n",
        "# texts = []"
      ],
      "metadata": {
        "id": "SOEy2AXcYzgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BOW\n",
        "def extract_bow(texts):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    print(\"BoW Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "    print(\"BoW Matrix:\\n\", X.toarray())\n",
        "    return X"
      ],
      "metadata": {
        "id": "yyfQpmXlY9V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF_IDF\n",
        "def extract_tfidf(texts):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    print(\"TF-IDF Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "    print(\"TF-IDF Matrix:\\n\", X.toarray())\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "G1IQdseJY9Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec\n",
        "def extract_word2vec(texts):\n",
        "    tokenized = [word_tokenize(text.lower()) for text in texts]\n",
        "    model = Word2Vec(sentences=tokenized, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    # Example: Get vector for 'nlp'\n",
        "    if 'nlp' in model.wv:\n",
        "        print(\"Vector for 'nlp':\", model.wv['nlp'])\n",
        "    else:\n",
        "        print(\"'nlp' not in vocabulary.\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "0EbZuAifZTJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe\n",
        "def extract_glove(texts):\n",
        "    glove = api.load(\"glove-wiki-gigaword-100\")\n",
        "    return average_embedding(texts, glove)"
      ],
      "metadata": {
        "id": "ppBwvAo4hyGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fast Text\n",
        "def extract_fasttext(texts):\n",
        "    ft = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "    return average_embedding(texts, ft)"
      ],
      "metadata": {
        "id": "KY4LEd4BhyUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT CLS Token Embeddings\n",
        "def extract_bert_cls(texts):\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # CLS token\n",
        "        embeddings.append(cls_embedding)\n",
        "\n",
        "    embeddings = torch.tensor(embeddings)\n",
        "    print(\"BERT Embeddings shape:\", embeddings.shape)\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "NQ4LIr_yfE1q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}